{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b7a30-c590-4c81-9fb8-6a6437fa5bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/seamless_communication/tree/main#quick-start\n",
    "'''\n",
    "!git clone https://github.com/facebookresearch/seamless_communication.git\n",
    "%cd seamless_communication\n",
    "pwd\n",
    "pip install .\n",
    "'''\n",
    "#https://huggingface.co/facebook/seamless-m4t-medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbc8d3a-fc02-4431-a64b-a98d325219c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 19 20:27:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   20C    P0    24W / 250W |      0MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72844982-cf33-4bfc-b4f4-209d16a92dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "VAdepthENV               /home/013907062/.conda/envs/VAdepthENV\n",
      "cmpe249                  /home/013907062/.conda/envs/cmpe249\n",
      "env_onmttf               /home/013907062/.conda/envs/env_onmttf\n",
      "hawaii                   /home/013907062/.conda/envs/hawaii\n",
      "hawaii_hf             *  /home/013907062/.conda/envs/hawaii_hf\n",
      "koen_base                /home/013907062/.conda/envs/koen_base\n",
      "newDepth                 /home/013907062/.conda/envs/newDepth\n",
      "seamless                 /home/013907062/.conda/envs/seamless\n",
      "test                     /home/013907062/.conda/envs/test\n",
      "wmt_infer                /home/013907062/.conda/envs/wmt_infer\n",
      "base                     /opt/ohpc/pub/apps/anaconda/3.9\n",
      "stylegan2                /opt/ohpc/pub/apps/anaconda/3.9/envs/stylegan2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb991f3b-d864-45ee-943b-a1be90866946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                         2.1.0+cu118\n",
      "torchaudio                    2.1.0+cu118\n",
      "torcheval                     0.0.7\n",
      "torchvision                   0.16.0+cu118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npip install torch==2.0.0\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip list | grep torch\n",
    "\n",
    "'''\n",
    "pip install torch==2.0.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dceb24c-8075-4c2d-b380-c0da228d1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d40850-9a15-4f9e-879c-2f9ad228acf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seamless_communication.models.inference import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd8818af-3a55-4e8a-b7da-d4a347296d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of the model 'seamlessM4T_medium'. Set `force=True` to download again.\n",
      "Using the cached tokenizer of the model 'seamlessM4T_medium'. Set `force=True` to download again.\n",
      "Using the cached checkpoint of the model 'vocoder_36langs'. Set `force=True` to download again.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Translator object with a multitask model, vocoder on the GPU.\n",
    "'''\n",
    "download model in CPU head node\n",
    "'''\n",
    "translator = Translator(\"seamlessM4T_medium\", vocoder_name_or_card=\"vocoder_36langs\", device=torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64cf8d0-0521-4fb3-a761-98f1c60efacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/013907062/OpenNMT-tf/scripts/seamless'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2046ab50-b88e-4695-a3e2-0560401a2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental health includes our emotional, psychological, and social well-being\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Kasama sa kalusugan ng isip ang ating emosyonal, sikolohikal, at panlipunang kagalingan\"\n",
    "tgt_lang = 'eng'\n",
    "src_lang = 'tgl'\n",
    "expected_text = \"Mental health includes our emotional, psychological, and social well-being\"\n",
    "\n",
    "translated_text, _, _ = translator.predict(input_text, \"t2tt\", tgt_lang, src_lang=src_lang)\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5e57159-f427-4a55-aaaf-11e149fd4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFSeamlessM4T:\n",
    "  def __init__(self, device=torch.device(\"cpu\")):\n",
    "    #translator = Translator(\"seamlessM4T_medium\", vocoder_name_or_card=\"vocoder_36langs\", device=torch.device(\"cuda:0\"))\n",
    "    _translator = Translator(\"seamlessM4T_medium\", vocoder_name_or_card=\"vocoder_36langs\", device=torch.device(device))\n",
    "\n",
    "  def _translate(self, text, src, tgt):\n",
    "    result, _, _ = translator.predict(input_text, \"t2tt\", tgt, src_lang=src)\n",
    "    return result\n",
    "\n",
    "  def translateText(self, text, src, tgt):\n",
    "    return self._translate(text, src, tgt)\n",
    "\n",
    "  def translateJson(json_text, src, tgt):\n",
    "    translated_json_text = \"\"\n",
    "    return translated_json_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "437d2f91-b3e6-41fa-a3bf-41e7176b9403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the cached checkpoint of the model 'seamlessM4T_medium'. Set `force=True` to download again.\n",
      "Using the cached tokenizer of the model 'seamlessM4T_medium'. Set `force=True` to download again.\n",
      "Using the cached checkpoint of the model 'vocoder_36langs'. Set `force=True` to download again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental health includes our emotional, psychological, and social well-being\n"
     ]
    }
   ],
   "source": [
    "translatorseamless = HFSeamlessM4T(torch.device(\"cuda:0\"))\n",
    "print(translatorseamless.translateText(input_text, src_lang, tgt_lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24471162-00a3-477e-a221-8643c43f4d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내 위에는 수십억 마리의 나비들이 있어요. 너무 많은 사람들 앞에서 연설을 할 수 있나요?\n"
     ]
    }
   ],
   "source": [
    "input_text = 'i have bilions butterflies in my stomach. Can I do speech in front of so many people?'\n",
    "src_lang = 'eng'\n",
    "tgt_lang = 'kor'\n",
    "\n",
    "print(translatorseamless.translateText(input_text, src_lang, tgt_lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1288c1f4-692e-4535-8489-3efd1433c958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
